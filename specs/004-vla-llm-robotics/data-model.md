# Data Model: Vision-Language-Action (VLA) for LLM-Robotics Integration

## Overview

This document defines the data structures, relationships, and flow for Module 4: Vision-Language-Action (VLA) for LLM-Robotics Integration. The system integrates voice processing, natural language understanding through LLMs, vision processing, and robot action execution.

## Core Data Entities

### 1. VoiceCommand
Represents a voice command received by the system.

**Fields:**
- `id` (string): Unique identifier for the command
- `audio_data` (binary): Raw audio data
- `transcribed_text` (string): Text from speech-to-text conversion
- `confidence` (float): Confidence score of transcription (0-1)
- `timestamp` (datetime): When command was received
- `user_id` (string): Identifier of the user issuing command (optional)
- `language` (string): Detected language of the command

### 2. CognitivePlan
The structured plan generated by the LLM from a natural language command.

**Fields:**
- `id` (string): Unique identifier for the plan
- `voice_command_id` (string): Reference to the original voice command
- `natural_language_input` (string): Original command text
- `action_sequence` (array[ActionStep]): Ordered sequence of actions to execute
- `context` (object): Environmental and state context used for planning
- `generated_at` (datetime): When the plan was generated
- `status` (string): Status of the plan (pending, executing, completed, failed)
- `estimated_duration` (float): Estimated time to execute the plan

### 3. ActionStep
A single action within a cognitive plan.

**Fields:**
- `id` (string): Unique identifier for the action step
- `cognitive_plan_id` (string): Reference to the parent cognitive plan
- `action_type` (string): Type of action (navigation, manipulation, perception, etc.)
- `parameters` (object): Action-specific parameters
- `preconditions` (array[Condition]): Conditions that must be true before execution
- `postconditions` (array[Condition]): Expected state after execution
- `priority` (integer): Priority level of the action
- `timeout` (float): Maximum time allowed for execution

### 4. Condition
Represents preconditions or postconditions for an action.

**Fields:**
- `id` (string): Unique identifier for the condition
- `type` (string): Type of condition (object_present, robot_at_location, etc.)
- `parameters` (object): Condition-specific parameters
- `is_met` (boolean): Whether the condition is currently satisfied

### 5. VisionPerception
Data from computer vision processing.

**Fields:**
- `id` (string): Unique identifier for the perception
- `timestamp` (datetime): When perception was captured
- `image_data` (binary): Raw image data
- `detected_objects` (array[DetectedObject]): List of recognized objects
- `environment_map` (object): Current environment representation
- `confidence_threshold` (float): Minimum confidence for object detection

### 6. DetectedObject
An object detected by the computer vision system.

**Fields:**
- `id` (string): Unique identifier for the object
- `vision_perception_id` (string): Reference to the parent vision perception
- `class_name` (string): Object class (e.g., "cup", "chair")
- `confidence` (float): Detection confidence score (0-1)
- `position` (object): 3D position in space (x, y, z)
- `orientation` (object): 3D orientation (quaternion: x, y, z, w)
- `bounding_box` (object): 2D bounding box in image (x, y, width, height)
- `properties` (object): Additional object properties

### 7. RobotState
Current state of the robot.

**Fields:**
- `id` (string): Unique identifier for the state record
- `timestamp` (datetime): When state was recorded
- `position` (object): 3D position (x, y, z)
- `orientation` (object): 3D orientation (quaternion: x, y, z, w)
- `joint_states` (object): Current joint angles/positions
- `battery_level` (float): Current battery level (0-1)
- `active_action` (string): Current action being executed (optional)
- `gripper_state` (string): State of any grippers (open, closed, etc.)

### 8. ExecutionLog
Log of action executions.

**Fields:**
- `id` (string): Unique identifier for the log entry
- `action_step_id` (string): Reference to the action being executed
- `start_time` (datetime): When execution started
- `end_time` (datetime): When execution ended (null if in progress)
- `status` (string): Execution status (success, failed, timeout)
- `error_message` (string): Error details if execution failed
- `actual_parameters` (object): Parameters actually used during execution
- `result_data` (object): Data returned by the action execution

## Data Relationships

```
VoiceCommand (1) -----> (1) CognitivePlan
CognitivePlan (1) -----> (n) ActionStep
ActionStep (n) -----> (n) Condition
CognitivePlan (1) -----> (n) ExecutionLog
ActionStep (n) -----> (n) ExecutionLog
VisionPerception (1) -----> (n) DetectedObject
RobotState (n) -- current state of --> Robot
```

## Data Flow

1. **Voice Input Flow**
   - User speaks → VoiceCommand is created → Whisper processes audio → transcribed_text is stored

2. **Cognitive Planning Flow**
   - VoiceCommand triggers LLM → CognitivePlan is generated → ActionStep sequence is created

3. **Vision Processing Flow**
   - Camera captures image → VisionPerception is created → DetectedObject records are generated

4. **Action Execution Flow**
   - ActionStep triggers robot → ExecutionLog records results → RobotState updates

## Data Validation Rules

### VoiceCommand Validation
- `transcribed_text` must not be empty
- `confidence` must be between 0 and 1
- `timestamp` must be within the last 60 seconds (for validity)

### CognitivePlan Validation
- `action_sequence` must contain at least one ActionStep
- Each ActionStep must have valid `action_type`
- `status` must be one of: pending, executing, completed, failed

### ActionStep Validation
- `action_type` must be a recognized action type
- `priority` must be a non-negative integer
- `timeout` must be positive

### DetectedObject Validation
- `confidence` must be between 0 and 1
- `position` must have valid x, y, z coordinates
- `class_name` must be in the recognized object classes list

## Data Storage and Performance Considerations

### Audio Data
- Audio data should be stored in compressed format
- Retention policy: Delete after 24 hours unless required for analysis
- Consider streaming processing to avoid large storage requirements

### Vision Data
- Raw images should be stored temporarily and processed rapidly
- Only store DetectedObject data for long-term analysis
- Use efficient encoding for image data

### Execution Logs
- Maintain logs for debugging and performance analysis
- Implement log rotation to manage storage
- Archive logs older than 30 days

### Robot State
- Store recent states in memory for quick access
- Persist states at regular intervals for recovery
- Implement state history for debugging purposes

## API Considerations

### Voice Command Interface
```
POST /voice-commands
{
  "audio_data": "...",
  "language": "en"
}
```

### Cognitive Plan Interface
```
GET /cognitive-plans/{id}
{
  "id": "...",
  "action_sequence": [...],
  "status": "executing"
}
```

### Vision Interface
```
POST /vision/perceive
{
  "image_data": "...",
  "timestamp": "2023-01-01T00:00:00Z"
}
```

## Security and Privacy

### Voice Data
- Encrypt audio data in transit and at rest
- Implement access controls for voice data
- Delete audio recordings after processing unless required

### Robot State Data
- Secure access to robot state information
- Implement audit logging for state changes
- Use authentication for all data access points

### LLM Interactions
- Secure API keys for LLM services
- Validate LLM outputs before execution
- Implement rate limiting for API usage